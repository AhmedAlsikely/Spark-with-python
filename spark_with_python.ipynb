{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-01-31T10:51:54.515080Z","iopub.execute_input":"2023-01-31T10:51:54.515468Z","iopub.status.idle":"2023-01-31T10:52:53.872804Z","shell.execute_reply.started":"2023-01-31T10:51:54.515437Z","shell.execute_reply":"2023-01-31T10:52:53.870924Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=882ffab3522d19591fe1d873fe9836d4e086b6b8cd86218cd772942b302b8e4d\n  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession","metadata":{"execution":{"iopub.status.busy":"2023-01-31T10:52:53.875019Z","iopub.execute_input":"2023-01-31T10:52:53.875767Z","iopub.status.idle":"2023-01-31T10:52:53.954849Z","shell.execute_reply.started":"2023-01-31T10:52:53.875724Z","shell.execute_reply":"2023-01-31T10:52:53.953804Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = r\"/kaggle/input/txtfileirish/irishlyrics.txt\"\nfile = open(dataset,'r')\nfile.readlines()[:5]","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:19:50.543689Z","iopub.execute_input":"2023-01-31T11:19:50.544129Z","iopub.status.idle":"2023-01-31T11:19:50.555499Z","shell.execute_reply.started":"2023-01-31T11:19:50.544094Z","shell.execute_reply":"2023-01-31T11:19:50.554067Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['Come all ye maidens young and fair\\n',\n 'And you that are blooming in your prime\\n',\n 'Always beware and keep your garden fair\\n',\n 'Let no man steal away your thyme\\n',\n 'For thyme it is a precious thing\\n']"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Create RDD using sparkContext.parallelize()**","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder.master(\"local\").appName('SparkByExamples.com').getOrCreate()\nsparkContext=spark.sparkContext\nrdd=sparkContext.parallelize([1,2,3,4,5])\nrddCollect = rdd.collect()\nprint(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\nprint(\"Action: First element: \"+str(rdd.first()))\nprint(rddCollect)\nprint(\"Action: sum: \"+str(rdd.sum()))","metadata":{"execution":{"iopub.status.busy":"2023-01-31T10:53:23.939302Z","iopub.execute_input":"2023-01-31T10:53:23.939711Z","iopub.status.idle":"2023-01-31T10:53:32.660911Z","shell.execute_reply.started":"2023-01-31T10:53:23.939678Z","shell.execute_reply":"2023-01-31T10:53:32.659453Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/01/31 10:53:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nNumber of Partitions: 1\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1:>                                                          (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"Action: First element: 1\n[1, 2, 3, 4, 5]\nAction: sum: 15\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Create RDD using sparkContext.textFile()**\n**Using textFile() method we can read a text (.txt) file into RDD.**","metadata":{}},{"cell_type":"code","source":"rdd2 = sparkContext.textFile(dataset)\nrdd2.take(5)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:01:33.150025Z","iopub.execute_input":"2023-01-31T11:01:33.150586Z","iopub.status.idle":"2023-01-31T11:01:33.273750Z","shell.execute_reply.started":"2023-01-31T11:01:33.150546Z","shell.execute_reply":"2023-01-31T11:01:33.272356Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['Come all ye maidens young and fair',\n 'And you that are blooming in your prime',\n 'Always beware and keep your garden fair',\n 'Let no man steal away your thyme',\n 'For thyme it is a precious thing']"},"metadata":{}}]},{"cell_type":"markdown","source":"# **RDD Transformations with example**","metadata":{}},{"cell_type":"code","source":"\nrdd3 = rdd2.flatMap(lambda line:line.split(\" \"))\nprint(\"result from flatMap\",rdd3.take(5))\n\nprint(\"*\"*50)\n\nrdd4 = rdd3.map(lambda word:(word,1))\nprint(\"result from Map\",rdd4.take(5))\n\nrdd5 = rdd4.reduceByKey(lambda a,b : a+b)\nrdd5.take(10)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:33:17.559392Z","iopub.execute_input":"2023-01-31T11:33:17.559802Z","iopub.status.idle":"2023-01-31T11:33:17.947913Z","shell.execute_reply.started":"2023-01-31T11:33:17.559770Z","shell.execute_reply":"2023-01-31T11:33:17.946652Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"result from flatMap ['Come', 'all', 'ye', 'maidens', 'young']\n**************************************************\nresult from Map [('Come', 1), ('all', 1), ('ye', 1), ('maidens', 1), ('young', 1)]\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[('Come', 6),\n ('all', 92),\n ('ye', 12),\n ('maidens', 1),\n ('young', 33),\n ('and', 370),\n ('fair', 19),\n ('And', 224),\n ('you', 97),\n ('that', 89)]"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Repartition and Coalesce**\n**Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition; first using repartition() method which shuffles data from all nodes also called full shuffle and second coalesce() method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.**","metadata":{}}]}